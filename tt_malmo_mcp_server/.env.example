# ============================================================
# LLM API Keys (Cloud Providers)
# ============================================================
# At least one of these should be set for the MCP server to work

# Google Gemini (free tier available)
GOOGLE_API_KEY=your_google_api_key_here

# Anthropic Claude (paid)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenRouter - gateway to many free models (recommended for testing)
# Sign up at https://openrouter.ai
OPENROUTER_API_KEY=your_openrouter_key_here

# Cerebras - ultra-fast inference (free tier: 30 RPM, 1M tokens/day)
# Sign up at https://cloud.cerebras.ai
CEREBRAS_API_KEY=your_cerebras_key_here

# Cloudflare Workers AI (free tier: 10k neurons/day)
# Get from Cloudflare dashboard
CLOUDFLARE_API_TOKEN=your_cloudflare_token_here
CLOUDFLARE_ACCOUNT_ID=your_account_id_here

# ============================================================
# Local vLLM Servers (NVIDIA GPU)
# ============================================================
# Each role runs on a different port with a specialized model.
# Start vLLM server with: python -m vllm.entrypoints.openai.api_server --model MODEL --port PORT

# Qwen 2.5 Coder 32B - Primary coding model (~18.5GB VRAM)
VLLM_CODER_URL=http://localhost:8000/v1
VLLM_CODER_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct-AWQ

# DeepSeek R1 Distill 70B - Deep reasoning (~38-40GB VRAM)
VLLM_REASONING_URL=http://localhost:8001/v1
VLLM_REASONING_MODEL=casperhansen/deepseek-r1-distill-llama-70b-awq

# Mistral Small 24B - Creative/Chat (~14.5GB VRAM)
VLLM_CREATIVE_URL=http://localhost:8002/v1
VLLM_CREATIVE_MODEL=casperhansen/mistral-small-24b-instruct-2501-awq

# Llama 3.1 8B - Ultra-fast (~5.5GB VRAM)
VLLM_FAST_URL=http://localhost:8003/v1
VLLM_FAST_MODEL=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4

# Default vLLM URL (for generic 'vllm' provider)
VLLM_DEFAULT_URL=http://localhost:8000/v1
VLLM_DEFAULT_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct-AWQ

# ============================================================
# Hybrid llama.cpp Server (CPU+GPU for massive models)
# ============================================================
# For models that don't fit in VRAM (405B, 671B)
# Start with: llama-server -m model.gguf -ngl 40 -c 8192 --port 8004

LLAMACPP_URL=http://localhost:8004/v1
LLAMACPP_MODEL=DeepSeek-R1-Q4_K_M

# ============================================================
# Database Configuration
# ============================================================
DATABASE_URL=postgresql://malmo:malmo_password@postgres:5432/malmo_benchmarks

# ============================================================
# Malmo Configuration
# ============================================================
MALMO_HOST=localhost
MALMO_PORT=9000
MALMO_AGENTS=5

# ============================================================
# Server Configuration
# ============================================================
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8000

# ============================================================
# Logging
# ============================================================
LOG_LEVEL=INFO
