# Core server dependencies
fastapi==0.109.0
uvicorn[standard]==0.27.0
websockets==12.0
pydantic==2.5.3

# LLM API clients
anthropic==0.18.1
google-generativeai==0.3.2

# Database for benchmarking
psycopg2-binary==2.9.9
sqlalchemy==2.0.25

# Utilities
python-dotenv==1.0.0

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
httpx==0.26.0

# Development
black==23.12.1
flake8==7.0.0
mypy==1.8.0

# HuggingFace Hub (for model downloading)
huggingface-hub>=0.20.0

# OpenAI-compatible client (for Ollama/llama.cpp servers)
openai>=1.0.0

# Dashboard
streamlit>=1.30.0
plotly>=5.18.0

# Note: For local LLM on Windows, use Ollama (https://ollama.com)
# vLLM does not support Windows natively
