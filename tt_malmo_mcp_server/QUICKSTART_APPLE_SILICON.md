# Quick Start Guide: Apple Silicon Mac (M1/M2/M3/M4)

This guide walks you through setting up and running the Malmo AI Benchmark on Apple Silicon Macs.

## Prerequisites

### 1. Install Homebrew
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Add to PATH (if not already done)
echo 'eval "$(/opt/homebrew/bin/brew shellenv)"' >> ~/.zshrc
source ~/.zshrc
```

### 2. Install Python 3.10+ (Recommended)
```bash
brew install python@3.11

# Verify version
python3.11 --version
```

### 3. Install Git
```bash
brew install git
```

---

## Step 1: Clone the Repository

```bash
# Clone from GitHub
git clone https://github.com/ricardotwumasi/tt_malmo.git
cd tt_malmo/tt_malmo_mcp_server
```

---

## Step 2: Set Up Python Virtual Environment

```bash
# Create virtual environment with Python 3.11
python3.11 -m venv venv

# Activate it
source venv/bin/activate

# Verify Python version
python --version  # Should show 3.11.x

# Upgrade pip
pip install --upgrade pip
```

---

## Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

This installs:
- FastAPI & Uvicorn (web server)
- Google Generative AI SDK (Gemini)
- Anthropic SDK (Claude)
- httpx (for OpenRouter, Cerebras, Cloudflare)
- pytest (testing)

---

## Step 4: Configure API Keys

### Option A: Using the setup script
```bash
./setup_api_key.sh
# Paste your Google API key when prompted
```

### Option B: Manual configuration
```bash
# Copy the example env file
cp .env.example .env

# Edit with your API keys
nano .env  # or use any text editor
```

**Required keys (at least one):**
```
# Google Gemini (free tier available)
GOOGLE_API_KEY=your_google_api_key_here

# OpenRouter (access to many free models)
OPENROUTER_API_KEY=your_openrouter_key_here
```

**Get free API keys:**
- **Gemini**: https://aistudio.google.com/apikey
- **OpenRouter**: https://openrouter.ai/keys (free tier available)
- **Cerebras**: https://cloud.cerebras.ai (free tier: 1M tokens/day)

---

## Step 5: Test the Setup

### Test Gemini Connection
```bash
python test_gemini.py
```

Expected output:
```
Testing Gemini 2.5 Flash Lite...
==================================================
API key loaded: AIzaSy...
Model: gemini-2.5-flash-lite

Sending prompt to Gemini...

Gemini Response:
--------------------------------------------------
[Some goal generated by the AI]
--------------------------------------------------

Gemini 2.5 Flash Lite is working!
```

### Run Unit Tests
```bash
pytest tests/ -v --tb=short
```

Expected: 95+ tests passing

---

## Step 6: Start the MCP Server

### Terminal 1: Start the server
```bash
cd tt_malmo_mcp_server
source venv/bin/activate

# Load environment variables
export $(cat .env | grep -v '^#' | xargs)

# Start server
python -m uvicorn mcp_server.server:app --reload --host 0.0.0.0 --port 8000
```

You should see:
```
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     Application startup complete.
```

---

## Step 7: Create and Monitor AI Agents

### Terminal 2: Interact with agents

#### Check server health
```bash
curl http://localhost:8000/health
```

#### Create a Gemini-powered agent
```bash
curl -X POST http://localhost:8000/agents \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Explorer_Alpha",
    "llm_type": "gemini",
    "role": 0,
    "traits": ["curious", "adventurous", "resourceful"]
  }'
```

Save the `agent_id` from the response.

#### Create an OpenRouter-powered agent (if you have the key)
```bash
curl -X POST http://localhost:8000/agents \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Gatherer_Beta",
    "llm_type": "openrouter",
    "role": 1,
    "traits": ["cooperative", "efficient", "cautious"]
  }'
```

#### List all agents
```bash
curl http://localhost:8000/agents
```

#### Start an agent's PIANO architecture
```bash
curl -X POST http://localhost:8000/agents/YOUR_AGENT_ID/start
```

#### Watch the server logs in Terminal 1
You'll see the agent making decisions:
```
[Explorer_Alpha] Decision: explore - Looking for resources and learning about the environment.
[Explorer_Alpha] Goal Generated: Find and gather wood for tools
```

#### Stop an agent
```bash
curl -X POST http://localhost:8000/agents/YOUR_AGENT_ID/stop
```

#### Delete an agent
```bash
curl -X DELETE http://localhost:8000/agents/YOUR_AGENT_ID
```

---

## Step 8: Human Oversight Dashboard

### Option A: Use curl to monitor
```bash
# Watch agent decisions in real-time (run in loop)
while true; do
  curl -s http://localhost:8000/agents
  sleep 5
done
```

### Option B: Open in browser
Navigate to: `http://localhost:8000/docs`

This opens the FastAPI Swagger UI where you can:
- View all API endpoints
- Create/start/stop agents interactively
- See request/response schemas

### Option C: WebSocket monitoring
Connect to `ws://localhost:8000/ws/agent/YOUR_AGENT_ID` to receive real-time updates.

---

## Understanding the PIANO Architecture

When an agent is running, 5 concurrent modules process information:

1. **Perception** - Processes environment observations
2. **Social Awareness** - Tracks nearby agents and relationships
3. **Goal Generation** - Uses LLM to generate new goals
4. **Action Awareness** - Monitors action success/failure
5. **Memory Consolidation** - Manages short/long-term memory

All information flows through the **Cognitive Controller** (bottleneck), which uses the LLM to make high-level decisions.

---

## Example: Multi-Agent Benchmark Session

```bash
# Create 3 agents with different providers
curl -X POST http://localhost:8000/agents \
  -H "Content-Type: application/json" \
  -d '{"name":"GeminiAgent","llm_type":"gemini","role":0,"traits":["curious"]}'

curl -X POST http://localhost:8000/agents \
  -H "Content-Type: application/json" \
  -d '{"name":"OpenRouterAgent","llm_type":"openrouter","role":1,"traits":["cooperative"]}'

# List agents
curl http://localhost:8000/agents

# Start all agents (replace with actual IDs)
curl -X POST http://localhost:8000/agents/AGENT_ID_1/start
curl -X POST http://localhost:8000/agents/AGENT_ID_2/start

# Watch them make decisions in Terminal 1 logs...

# Stop all when done
curl -X POST http://localhost:8000/agents/AGENT_ID_1/stop
curl -X POST http://localhost:8000/agents/AGENT_ID_2/stop
```

---

## Troubleshooting

### "GOOGLE_API_KEY environment variable not set"
```bash
# Make sure to export from .env
export $(cat .env | grep -v '^#' | xargs)

# Or add to your shell profile
echo 'export GOOGLE_API_KEY=your_key_here' >> ~/.zshrc
source ~/.zshrc
```

### "Port 8000 already in use"
```bash
# Find and kill the process
lsof -ti:8000 | xargs kill -9

# Or use a different port
python -m uvicorn mcp_server.server:app --port 8001
```

### Python version warnings
```bash
# Install Python 3.11 and recreate venv
brew install python@3.11
rm -rf venv
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Module not found errors
```bash
# Make sure venv is activated
source venv/bin/activate

# Reinstall dependencies
pip install -r requirements.txt
```

---

## Handling Rate Limits

### Free Tier Limits
| Provider | Rate Limit | Daily Limit |
|----------|------------|-------------|
| Gemini | 10 req/min | 1,500/day |
| OpenRouter | 20 req/min | 50-1000/day |
| Cerebras | 30 req/min | 1M tokens/day |

### Strategy: Spread Load Across Providers
```bash
# Create agents with different providers
curl -X POST http://localhost:8000/agents \
  -d '{"name":"Agent1","llm_type":"gemini","role":0}'

curl -X POST http://localhost:8000/agents \
  -d '{"name":"Agent2","llm_type":"openrouter","role":1}'

curl -X POST http://localhost:8000/agents \
  -d '{"name":"Agent3","llm_type":"cerebras","role":2}'
```

### Strategy: Increase Decision Interval
Edit `mcp_server/agent_manager.py` to increase decision interval:
```python
# Change from 5 seconds to 10 seconds
cognitive_controller = CognitiveController(llm_adapter, decision_interval=10.0)
```

---

## Part 2: Full Malmo/Minecraft Integration

### Step 1: Run the Malmo Setup Script
```bash
cd tt_malmo_mcp_server
./setup_malmo_apple_silicon.sh
```

This script will:
1. Install Rosetta 2 (if needed)
2. Install Java 8 x86_64 via Homebrew
3. Build Minecraft with Malmo mod
4. Install MalmoEnv Python package

### Step 2: Test Without Minecraft (Decision Loop Only)
```bash
source venv/bin/activate
export $(cat .env | grep -v '^#' | xargs)

# Test agent decision-making without Malmo
python test_malmo_integration.py --no-malmo
```

Expected output:
```
[Step 1] Observation:
  Location: (10.0, 64.0, 5.0)
  ...
  Decision: explore
  Reasoning: My current goal is to craft a wooden pickaxe...

[Step 2] Observation:
  ...
  Decision: gather wood
  Reasoning: To craft a wooden pickaxe, I need wood...
```

### Step 3: Launch Minecraft with Malmo

**Terminal 1 - Start Minecraft:**
```bash
./launch_malmo.sh 9000
```

Wait for Minecraft to fully load (shows main menu).

**Terminal 2 - Start MCP Server:**
```bash
source venv/bin/activate
export $(cat .env | grep -v '^#' | xargs)
python -m uvicorn mcp_server.server:app --reload
```

**Terminal 3 - Run Full Integration Test:**
```bash
source venv/bin/activate
export $(cat .env | grep -v '^#' | xargs)

# Full test with Malmo
python test_malmo_integration.py --port 9000 --steps 100 --llm gemini
```

---

## Next Steps

1. **Connect to Malmo/Minecraft** (requires Java 8 under Rosetta - see MACOS_SETUP.md)
2. **Run benchmarks** using the benchmarking system
3. **Compare agents** across different LLM providers
4. **Customize agent traits** and observe behavioral differences

---

## Quick Reference

| Command | Description |
|---------|-------------|
| `source venv/bin/activate` | Activate Python environment |
| `python -m uvicorn mcp_server.server:app --reload` | Start server |
| `curl http://localhost:8000/health` | Check server status |
| `curl http://localhost:8000/agents` | List all agents |
| `curl -X POST .../agents -d '{...}'` | Create agent |
| `curl -X POST .../agents/ID/start` | Start agent |
| `curl -X POST .../agents/ID/stop` | Stop agent |
| `pytest tests/ -v` | Run tests |

---

**Status**: Ready for local testing without Minecraft
**Next**: Connect to Malmo for full environment integration
